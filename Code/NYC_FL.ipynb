{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYC_FL.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01ekVSe4McOZ"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69Zus3q5KKpv"
      },
      "source": [
        "!pip install --quiet --upgrade tensorflow_federated_nightly\n",
        "!pip install --quiet --upgrade nest_asyncio\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0AtRRZwKfnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "ec8f53e1-bede-41f7-a6a1-2bf8249bbf6b"
      },
      "source": [
        "import collections\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "baseURL = '/content/gdrive/My Drive/NYC Dataset/';\n",
        "directory = baseURL + 'test'\n",
        "\n",
        "# Test the TFF is working:\n",
        "tff.federated_computation(lambda: 'Hello, World!')()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:43: UserWarning: You are currently using a nightly version of TensorFlow (2.4.0-dev20200922). \n",
            "TensorFlow Addons offers no support for the nightly versions of TensorFlow. Some things might work, some other might not. \n",
            "If you encounter a bug, do not file an issue on GitHub.\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'Hello, World!'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRSBBlWyLG9h"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiraMJg9uOb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bd845c7d-9072-46d5-ef12-b26e94566280"
      },
      "source": [
        "# read the dataset from Drive\n",
        "df = pd.read_csv(baseURL + \"trips_with_zones_final.csv\")\n",
        "df = df.head(10000000)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>medallion</th>\n",
              "      <th>pickup_week_day</th>\n",
              "      <th>pickup_hour</th>\n",
              "      <th>pickup_day</th>\n",
              "      <th>pickup_month</th>\n",
              "      <th>dropoff_week_day</th>\n",
              "      <th>dropoff_hour</th>\n",
              "      <th>dropoff_day</th>\n",
              "      <th>dropoff_month</th>\n",
              "      <th>pickup_location_id</th>\n",
              "      <th>dropoff_location_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00005007A9F30E289E760362F69E4EAD</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>162.0</td>\n",
              "      <td>262.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00005007A9F30E289E760362F69E4EAD</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>262.0</td>\n",
              "      <td>239.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00005007A9F30E289E760362F69E4EAD</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>239.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00005007A9F30E289E760362F69E4EAD</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>236.0</td>\n",
              "      <td>41.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00005007A9F30E289E760362F69E4EAD</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>41.0</td>\n",
              "      <td>211.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          medallion  ...  dropoff_location_id\n",
              "0  00005007A9F30E289E760362F69E4EAD  ...                262.0\n",
              "1  00005007A9F30E289E760362F69E4EAD  ...                239.0\n",
              "2  00005007A9F30E289E760362F69E4EAD  ...                  0.0\n",
              "3  00005007A9F30E289E760362F69E4EAD  ...                 41.0\n",
              "4  00005007A9F30E289E760362F69E4EAD  ...                211.0\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggIfgQxKuOcC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "4a248d3b-8b73-4f2b-d7f3-8f9bcaaedbd6"
      },
      "source": [
        "# Check dtypes of the attributes\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "medallion               object\n",
              "pickup_week_day          int64\n",
              "pickup_hour              int64\n",
              "pickup_day               int64\n",
              "pickup_month             int64\n",
              "dropoff_week_day         int64\n",
              "dropoff_hour             int64\n",
              "dropoff_day              int64\n",
              "dropoff_month            int64\n",
              "pickup_location_id     float64\n",
              "dropoff_location_id    float64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_EEwnknuOcG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "0904a854-cabc-4bac-ffc9-d268c1e214a1"
      },
      "source": [
        "# Cast the columns type to int32\n",
        "dictionary = {'pickup_week_day': 'int32', 'pickup_hour': 'int32', 'pickup_day': 'int32', 'pickup_month': 'int32', 'dropoff_week_day': 'int32', 'dropoff_hour': 'int32', 'dropoff_day': 'int32', 'dropoff_month': 'int32', 'pickup_location_id':'int32', 'dropoff_location_id':'int32'}\n",
        "df = df.astype(dictionary, copy=True)\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "medallion              object\n",
              "pickup_week_day         int32\n",
              "pickup_hour             int32\n",
              "pickup_day              int32\n",
              "pickup_month            int32\n",
              "dropoff_week_day        int32\n",
              "dropoff_hour            int32\n",
              "dropoff_day             int32\n",
              "dropoff_month           int32\n",
              "pickup_location_id      int32\n",
              "dropoff_location_id     int32\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoTlUvLIPQFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "333c768f-1188-4d8e-f4d1-2c62aabff0b3"
      },
      "source": [
        "df.medallion.value_counts().loc[df.medallion.value_counts().index[100]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep1BmXDVPxC1"
      },
      "source": [
        "Because there are too many taxis (over 9000) it is better to take the 100 taxi with the major number of records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jsCxF6RPQC5"
      },
      "source": [
        "# Pick taxis with at least 1000 records\n",
        "count = df.medallion.value_counts()\n",
        "\n",
        "medallions = count.loc[count.index[:100]].index # count >= 1000\n",
        "test_medallions = count.loc[count.index[100:105]].index\n",
        "val_medallions = count.loc[count.index[105:110]].index\n",
        "\n",
        "df_test = df.loc[df.medallion.isin(test_medallions)].copy()\n",
        "df_val = df.loc[df.medallion.isin(val_medallions)].copy()\n",
        "df = df.loc[df.medallion.isin(medallions)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oH6GxKzwc7K"
      },
      "source": [
        "We can use the other taxis to create a local test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WESdy69_Ljgq"
      },
      "source": [
        "# function to remove duplicates\n",
        "def create_sequence(locations): \n",
        "  # Flatten the list of places\n",
        "  sequence = np.reshape(locations.values, [-1])\n",
        "\n",
        "  # Create a temporary array of the same lenght of the sequece of locations\n",
        "  copy = np.zeros(sequence.shape[0], dtype=np.int32)\n",
        "\n",
        "  # Copy the sequence of location in the copy array but shifted right by 1 position\n",
        "  # The last location does not need to be copied, it can't be a duplicate\n",
        "  copy[1:] = sequence[:sequence.shape[0]-1]\n",
        "\n",
        "  # Where we get 0 it can be a possible duplicated\n",
        "  duplicated = sequence - copy\n",
        "\n",
        "  # indices where the subtraction gives 0\n",
        "  idx = np.where(duplicated == 0)[0]\n",
        "\n",
        "  # Find where the position of the zeros are even\n",
        "  even = idx%2 == 0\n",
        "\n",
        "  # List the indices where the position is even and the subtraction gave 0\n",
        "  to_drop = idx[even]\n",
        "\n",
        "  # Remove the duplicates\n",
        "  clean_sequence = np.delete(sequence, to_drop)\n",
        "  return clean_sequence, to_drop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHECUQleRxHH"
      },
      "source": [
        "Now we need to create the location sequence for each user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzze3vF-R8OZ"
      },
      "source": [
        "def df_to_location_sequence(df):\n",
        "  \n",
        "  # take just the columns we need\n",
        "  locations = df[['pickup_location_id','dropoff_location_id']].copy()\n",
        "  locations = locations.astype('int32')\n",
        "\n",
        "\n",
        "  # define the indices to keep trace of the locations\n",
        "  x = np.arange(0, locations.values.shape[0])\n",
        "\n",
        "  pos = np.array([x,x]).T\n",
        "  pos = np.reshape(pos, [-1])\n",
        "\n",
        "  # Represent whether the location is a pickup or a dropoff\n",
        "  pick = np.zeros(locations.values.shape[0], dtype=int)\n",
        "  drop = np.ones(locations.values.shape[0], dtype=int)\n",
        "\n",
        "  loc = np.array([pick,drop]).T\n",
        "  loc = np.reshape(loc, [-1])\n",
        "\n",
        "  # Generate the sequence of places\n",
        "  sequence, duplicates = create_sequence(locations)\n",
        "\n",
        "  # We use now the indices of the duplicated locations to clean also the array of rows and the array of location types\n",
        "  pos = np.delete(pos, duplicates)\n",
        "  loc = np.delete(loc, duplicates)\n",
        "\n",
        "  # Select the indices of records we want the pickup location\n",
        "  pick_pos = pos[pos[loc == 0]]\n",
        "\n",
        "  # Select the indices of records we want the dropoff location\n",
        "  drop_pos = pos[pos[loc == 1]]\n",
        "\n",
        "  \n",
        "  records_pick = df.iloc[pick_pos][['medallion', 'pickup_location_id', 'pickup_week_day',\t'pickup_hour',\t'pickup_day',\t'pickup_month']]\n",
        "  records_pick = records_pick.rename(columns={'pickup_location_id': 'location_id', 'pickup_week_day':'week_day' ,\t'pickup_hour':'hour' ,\t'pickup_day':\t'day' ,\t'pickup_month':'month' })\n",
        "\n",
        "  idx_drop = np.nonzero(loc == 0)[0]\n",
        "  records_drop = df.iloc[drop_pos][['medallion', 'dropoff_location_id', 'dropoff_week_day',\t'dropoff_hour',\t'dropoff_day',\t'dropoff_month']]\n",
        "  records_drop = records_drop.rename(columns={'dropoff_location_id': 'location_id', 'dropoff_week_day':'week_day' ,\t'dropoff_hour':'hour' ,\t'dropoff_day':\t'day' ,\t'dropoff_month':'month' })\n",
        "\n",
        "  locations_sequence = pd.concat([records_pick, records_drop])\n",
        "\n",
        "  # reset the index\n",
        "  locations_sequence.reset_index(inplace=True)\n",
        "\n",
        "  # From hour to sin-cos representation\n",
        "  locations_sequence['hour_sin'] = np.sin(locations_sequence.hour*(2.*np.pi/24))\n",
        "  locations_sequence['hour_cos'] = np.cos(locations_sequence.hour*(2.*np.pi/24))\n",
        "\n",
        "  locations_sequence['week_day_sin'] = np.sin(locations_sequence.week_day*(2.*np.pi/7))\n",
        "  locations_sequence['week_day_cos'] = np.cos(locations_sequence.week_day*(2.*np.pi/7))\n",
        "\n",
        "\n",
        "  # Drop the original column\n",
        "  locations_sequence.drop(['hour'], axis=1, inplace=True)\n",
        "  \n",
        "\n",
        "  # Helper function to encode the day_type\n",
        "  def is_weekend(days):\n",
        "    weekends = np.zeros(len(days))\n",
        "    weekends[((days == 5) | (days == 6))] = 1\n",
        "    return weekends\n",
        "\n",
        "  # Apply the helper function to all the records\n",
        "  locations_sequence['weekend'] = is_weekend(locations_sequence['week_day'])\n",
        "\n",
        "  # the column is not needed anymore\n",
        "  locations_sequence.drop(['week_day'], axis=1, inplace=True)\n",
        "\n",
        "  # Correct the weekend feature type\n",
        "  dictionary = {'weekend': 'int32'}\n",
        "  locations_sequence = locations_sequence.astype(dictionary, copy=True)\n",
        "  \n",
        "  return locations_sequence, pos, loc\n",
        "\n",
        "# Call the function\n",
        "locations_sequence, pos, loc = df_to_location_sequence(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbQmLj9kSYqp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07af026d-8765-4323-8204-953df7b1eccc"
      },
      "source": [
        "# List the df for each user\n",
        "users_locations = []\n",
        "\n",
        "# For each user\n",
        "for medallion in tqdm(medallions):\n",
        "  # Call the function\n",
        "  locations_sequence, pos, loc = df_to_location_sequence(df.loc[df.medallion == medallion].copy())\n",
        "  # Add the sequence df of the user to the list\n",
        "  users_locations.append(locations_sequence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 34.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltjLgzdf1LYY"
      },
      "source": [
        "test_locations_sequence, pos, loc = df_to_location_sequence(df_test)\n",
        "val_locations_sequence, pos, loc = df_to_location_sequence(df_val)\n",
        "\n",
        "test_locations_sequence.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
        "val_locations_sequence.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
        "\n",
        "# Split the data into chunks\n",
        "N = 17\n",
        "\n",
        "# Test\n",
        "# Get a list of dataframes of length n records \n",
        "list_test = [test_locations_sequence[i:i+N] for i in range(0, test_locations_sequence.shape[0], N)]\n",
        "\n",
        "# Test\n",
        "# Get a list of dataframes of length n records \n",
        "list_val = [val_locations_sequence[i:i+N] for i in range(0, val_locations_sequence.shape[0], N)]\n",
        "list_test[0]\n",
        "\n",
        "if len(list_val[-1]) < N:\n",
        "  diff_val = 1\n",
        "else:\n",
        "  diff_val = 0\n",
        "\n",
        "if len(list_test[-1]) < N:\n",
        "  diff_test = 1\n",
        "else:\n",
        "  diff_test = 0\n",
        "\n",
        "\n",
        "# Define the input features of the  dataset\n",
        "val_input_dict = {\n",
        "  'start_place':np.array([list_val[i]['location_id'].values[:-1] for i in range(0, len(list_val)-diff_val)]), \n",
        "  'start_hour_sin':np.array([list_val[i]['hour_sin'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
        "  'start_hour_cos':np.array([list_val[i]['hour_cos'].values[:-1] for i in range(0, len(list_val)-diff_val)]), \n",
        "  'weekend':np.array([list_val[i]['weekend'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
        "  'week_day_sin':np.array([list_val[i]['week_day_sin'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
        "  'week_day_cos':np.array([list_val[i]['week_day_cos'].values[:-1] for i in range(0, len(list_val)-diff_val)]),\n",
        "}\n",
        "\n",
        "# Define the input features of the  dataset\n",
        "test_input_dict = {\n",
        "  'start_place':np.array([list_test[i]['location_id'].values[:-1] for i in range(0, len(list_test)-diff_test)]), \n",
        "  'start_hour_sin':np.array([list_test[i]['hour_sin'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
        "  'start_hour_cos':np.array([list_test[i]['hour_cos'].values[:-1] for i in range(0, len(list_test)-diff_test)]), \n",
        "  'weekend':np.array([list_test[i]['weekend'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
        "  'week_day_sin':np.array([list_test[i]['week_day_sin'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
        "  'week_day_cos':np.array([list_test[i]['week_day_cos'].values[:-1] for i in range(0, len(list_test)-diff_test)]),\n",
        "}\n",
        "\n",
        "# Create training examples / targets, we are going to predict the next location\n",
        "trips_dataset_val = tf.data.Dataset.from_tensor_slices((val_input_dict, np.array([list_val[i]['location_id'].values[1:] for i in range(0, len(list_val)-diff_val)])) )\n",
        "trips_dataset_test = tf.data.Dataset.from_tensor_slices((test_input_dict, np.array([list_test[i]['location_id'].values[1:] for i in range(0, len(list_test)-diff_test)])) )\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Create the dataset by creating batches\n",
        "# Uncomment the shuffle function in case we want to shuffle the sequences\n",
        "val_dataset = trips_dataset_val.batch(BATCH_SIZE, drop_remainder=True) #.shuffle(BUFFER_SIZE)\n",
        "test_dataset = trips_dataset_test.batch(BATCH_SIZE, drop_remainder=True) #.shuffle(BUFFER_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEMnEoqY9CAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "37048dd7-05ce-4780-fda7-70351e948503"
      },
      "source": [
        "sizes = []\n",
        "# Number of locations for each user\n",
        "for user_df in users_locations:\n",
        "  sizes.append(user_df.shape[0])\n",
        "\n",
        "print('Mean number of locations: ', np.mean(np.array(sizes)))\n",
        "print('Max number of locations: ', np.max(np.array(sizes)))\n",
        "print('Min number of locations: ', np.min(np.array(sizes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean number of locations:  2424.41\n",
            "Max number of locations:  2802\n",
            "Min number of locations:  2260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dklBpuI4YSbf"
      },
      "source": [
        "Create the validation and test sets for each user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHDBtZVwYXWT"
      },
      "source": [
        "# List the dfs fo train, val and test for each user\n",
        "users_locations_train = []\n",
        "users_locations_val = []\n",
        "users_locations_test = []\n",
        "\n",
        "for user_df in users_locations:\n",
        "  # Split in train, test and validation\n",
        "  train, test = train_test_split(user_df, test_size=0.2, shuffle=False)\n",
        "  train, val = train_test_split(train, test_size=0.2, shuffle=False)\n",
        "\n",
        "  # Append the sets\n",
        "  users_locations_train.append(train)\n",
        "  users_locations_val.append(val)\n",
        "  users_locations_test.append(test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8IIiHFcBJJI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fafe94bc-e851-461f-aa9c-f2b59fd1522f"
      },
      "source": [
        "sizes = []\n",
        "# Number of locations for each user in the validation set\n",
        "for user_df in users_locations_val:\n",
        "  sizes.append(user_df.shape[0])\n",
        "\n",
        "print('Mean number of locations: ', np.mean(np.array(sizes)))\n",
        "print('Max number of locations: ', np.max(np.array(sizes)))\n",
        "print('Min number of locations: ', np.min(np.array(sizes)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean number of locations:  388.24\n",
            "Max number of locations:  449\n",
            "Min number of locations:  362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYtsvsO0aUi2"
      },
      "source": [
        "Create sequences for each client"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15nXfT6MT9AF"
      },
      "source": [
        "# Merge back the dataframes\n",
        "df_train = pd.concat(users_locations_train)\n",
        "df_train.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
        "\n",
        "# Merge back the dataframes\n",
        "df_val = pd.concat(users_locations_val)\n",
        "df_val.drop(['index', 'day', 'month'], axis=1, inplace=True)\n",
        "\n",
        "# Merge back the dataframes\n",
        "df_test = pd.concat(users_locations_test)\n",
        "df_test.drop(['index', 'day', 'month'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z77E_KMGvy9R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0f9b691-d41a-42a9-b92f-b57cc8eae4e3"
      },
      "source": [
        "# list of unique medallions\n",
        "medallions_list = df_train.medallion.unique()\n",
        "\n",
        "# number of unique medallions\n",
        "medallions_num = len(medallions_list)\n",
        "print(medallions_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTQqAoZaaXL1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "d2ac02ef-3ca5-4f05-a02f-9722c1bbcb32"
      },
      "source": [
        "# Split the data into chunks of N+1\n",
        "N = 17\n",
        "\n",
        "# dictionary of list of df \n",
        "df_dictionary = {}\n",
        "\n",
        "for medallion in tqdm(medallions_list):\n",
        "\n",
        "  # Get the records of the user\n",
        "  user_df_train = df_train.loc[df_train.medallion == medallion].copy()\n",
        "  user_df_val = df_val.loc[df_val.medallion == medallion].copy()\n",
        "  user_df_test = df_test.loc[df_test.medallion == medallion].copy()\n",
        "\n",
        "  # Get a list of dataframes of length N records \n",
        "  user_list_train = [user_df_train[i:i+N] for i in range(0, user_df_train.shape[0], N)]\n",
        "  user_list_val = [user_df_val[i:i+N] for i in range(0, user_df_val.shape[0], N)]\n",
        "  user_list_test = [user_df_test[i:i+N] for i in range(0, user_df_test.shape[0], N)]\n",
        "\n",
        "  # Save the list of dataframes into a dictionary\n",
        "  df_dictionary[medallion] = {\n",
        "      'train': user_list_train,\n",
        "      'val': user_list_val,\n",
        "      'test': user_list_test\n",
        "  }\n",
        "\n",
        "'''\n",
        "# Validation\n",
        "# Get a list of dataframes of length n records \n",
        "list_val = [val[i:i+N] for i in range(0, val.shape[0], N)]\n",
        "\n",
        "# Test\n",
        "# Get a list of dataframes of length n records \n",
        "list_test = [test[i:i+N] for i in range(0, test.shape[0], N)]\n",
        "list_test[0]'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:04<00:00, 22.15it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Validation\\n# Get a list of dataframes of length n records \\nlist_val = [val[i:i+N] for i in range(0, val.shape[0], N)]\\n\\n# Test\\n# Get a list of dataframes of length n records \\nlist_test = [test[i:i+N] for i in range(0, test.shape[0], N)]\\nlist_test[0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38O5t3bizJVM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b5fd5ea3-e2cf-4ea3-e32e-756dc13cde65"
      },
      "source": [
        "df_train.columns.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['medallion', 'location_id', 'hour_sin', 'hour_cos', 'week_day_sin',\n",
              "       'week_day_cos', 'weekend'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pbQL_4svmnY"
      },
      "source": [
        "# Create the dictionary to create a clientData\n",
        "columns_names = df_train.columns.values[1:]\n",
        "\n",
        "# Takes a dictionary with train, validation an test sets and the desired set type\n",
        "def create_clients_dict(df_dictionary, set_type):\n",
        "  \n",
        "  dataset_dict = {}\n",
        "\n",
        "  for medallion in tqdm(medallions_list):\n",
        "\n",
        "    c_data = collections.OrderedDict()\n",
        "    values = df_dictionary[medallion][set_type]\n",
        "\n",
        "    # If the last dataframe of the list is not complete\n",
        "    if len(values[-1]) < N:\n",
        "      diff = 1\n",
        "    else:\n",
        "      diff = 0\n",
        "\n",
        "    if len(values) > 0:\n",
        "      for header in columns_names:\n",
        "        #c_data[header] = values[header].values.tolist()\n",
        "        c_data[header] = [values[i][header].values for i in range(0, len(values)-diff)] #[:-1]\n",
        "        #c_data['y'] = values['dropoff_location_id'].values.tolist()\n",
        "      dataset_dict[medallion] = c_data\n",
        "      \n",
        "  return dataset_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jc41ZQRM066G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a1a09f75-4f96-491a-b656-2ea6bdf76e7d"
      },
      "source": [
        "# Generate the dictionaries for each set\n",
        "clients_train_dict = create_clients_dict(df_dictionary, 'train')\n",
        "clients_val_dict = create_clients_dict(df_dictionary, 'val')\n",
        "clients_test_dict = create_clients_dict(df_dictionary, 'test')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 45.31it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 249.91it/s]\n",
            "100%|██████████| 100/100 [00:00<00:00, 132.11it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_1c1VkYu9TM"
      },
      "source": [
        "# Convert the dictionary to a dataset\n",
        "client_train_data = tff.simulation.FromTensorSlicesClientData(clients_train_dict)\n",
        "client_val_data = tff.simulation.FromTensorSlicesClientData(clients_val_dict)\n",
        "client_test_data = tff.simulation.FromTensorSlicesClientData(clients_test_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syED5r3_A-Ej",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eeb96625-1fd8-473c-f0c5-c4caf9583d5c"
      },
      "source": [
        "client_train_data.create_tf_dataset_for_client(medallions_list[0]).element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('location_id',\n",
              "              TensorSpec(shape=(17,), dtype=tf.int32, name=None)),\n",
              "             ('hour_sin',\n",
              "              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n",
              "             ('hour_cos',\n",
              "              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n",
              "             ('week_day_sin',\n",
              "              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n",
              "             ('week_day_cos',\n",
              "              TensorSpec(shape=(17,), dtype=tf.float64, name=None)),\n",
              "             ('weekend', TensorSpec(shape=(17,), dtype=tf.int32, name=None))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqWe5BAN20MQ"
      },
      "source": [
        "Retrieve and example dataset from client_data to take a look at its structure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UIBQ2Gv3MA5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "d7aafc51-b3eb-4583-8ca1-59a930afeb00"
      },
      "source": [
        "example_dataset = client_train_data.create_tf_dataset_for_client(\n",
        "    client_train_data.client_ids[1])\n",
        "\n",
        "example_element = next(iter(example_dataset))\n",
        "example_element"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('location_id', <tf.Tensor: shape=(17,), dtype=int32, numpy=\n",
              "              array([249, 249,  68, 133,   0, 186, 186, 143, 229,  45, 137,   0, 233,\n",
              "                       0, 114, 164,   0], dtype=int32)>),\n",
              "             ('hour_sin', <tf.Tensor: shape=(17,), dtype=float64, numpy=\n",
              "              array([ 0.00000000e+00,  0.00000000e+00,  7.07106781e-01,  7.07106781e-01,\n",
              "                      5.00000000e-01,  5.00000000e-01,  5.00000000e-01,  2.58819045e-01,\n",
              "                      1.22464680e-16,  1.22464680e-16,  1.22464680e-16,  1.22464680e-16,\n",
              "                     -2.58819045e-01, -2.58819045e-01, -2.58819045e-01, -2.58819045e-01,\n",
              "                     -2.58819045e-01])>),\n",
              "             ('hour_cos', <tf.Tensor: shape=(17,), dtype=float64, numpy=\n",
              "              array([ 1.        ,  1.        ,  0.70710678,  0.70710678, -0.8660254 ,\n",
              "                     -0.8660254 , -0.8660254 , -0.96592583, -1.        , -1.        ,\n",
              "                     -1.        , -1.        , -0.96592583, -0.96592583, -0.96592583,\n",
              "                     -0.96592583, -0.96592583])>),\n",
              "             ('week_day_sin', <tf.Tensor: shape=(17,), dtype=float64, numpy=\n",
              "              array([0.78183148, 0.78183148, 0.78183148, 0.78183148, 0.78183148,\n",
              "                     0.78183148, 0.78183148, 0.78183148, 0.78183148, 0.78183148,\n",
              "                     0.78183148, 0.78183148, 0.78183148, 0.78183148, 0.78183148,\n",
              "                     0.78183148, 0.78183148])>),\n",
              "             ('week_day_cos', <tf.Tensor: shape=(17,), dtype=float64, numpy=\n",
              "              array([0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898,\n",
              "                     0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898,\n",
              "                     0.6234898, 0.6234898, 0.6234898, 0.6234898, 0.6234898])>),\n",
              "             ('weekend',\n",
              "              <tf.Tensor: shape=(17,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coraDX-adRjf"
      },
      "source": [
        "*Shuffle the individual examples, organize them into batches and renames the target feature from `dropoff_location_id ` to y for use with Keras. We also throw in a repeat over the data set to run several epochs.*\n",
        "\n",
        "Because `tff.learning.from_keras_model` wants as input_spec a dictionary of 2 elements (x,y) and we have multiple inputs, we have to make also x a ditionary.\n",
        "\n",
        "In this way we will be able to process each feature separately in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIVbzPidc4bi"
      },
      "source": [
        "NUM_CLIENTS = medallions_num\n",
        "NUM_EPOCHS = 4\n",
        "BATCH_SIZE = 16\n",
        "SHUFFLE_BUFFER = 100\n",
        "PREFETCH_BUFFER = 5\n",
        "\n",
        "def preprocess(dataset):\n",
        "  def batch_format_fn(element):\n",
        "    \"\"\"Flatten a batch `pixels` and return the features as an `OrderedDict`.\"\"\"\n",
        "    return collections.OrderedDict(\n",
        "        x=collections.OrderedDict(\n",
        "          start_place=tf.reshape(element['location_id'][:, :-1], [-1, N-1]),\n",
        "          start_hour_sin=tf.reshape(element['hour_sin'][:, :-1], [-1, N-1]),\n",
        "          start_hour_cos=tf.reshape(element['hour_cos'][:, :-1], [-1, N-1]),\n",
        "          week_day_sin=tf.reshape(element['week_day_sin'][:, :-1], [-1, N-1]),\n",
        "          week_day_cos=tf.reshape(element['week_day_cos'][:, :-1], [-1, N-1]),\n",
        "          weekend=tf.reshape(element['weekend'][:, :-1], [-1, N-1])\n",
        "          ),\n",
        "        y=tf.reshape(element['location_id'][:, 1:], [-1, N-1]))\n",
        "  return dataset.repeat(NUM_EPOCHS).batch(BATCH_SIZE, drop_remainder=True).map(batch_format_fn).prefetch(PREFETCH_BUFFER) # .shuffle(SHUFFLE_BUFFER)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irPtpimwe7Y-"
      },
      "source": [
        "Test the preprocessing on a single client dataset\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fklCZxNpeXg4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "de02d78a-2053-45c4-c4e9-038eb36f6ed9"
      },
      "source": [
        "preprocessed_example_dataset = preprocess(example_dataset)\n",
        "sample_batch = tf.nest.map_structure(lambda x: x.numpy(),\n",
        "                                     next(iter(preprocessed_example_dataset)))\n",
        "\n",
        "sample_batch['x']['start_place'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16, 16)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJLyl_cr61jR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3218507f-ca83-4d43-801e-f9ae4603c695"
      },
      "source": [
        "preprocessed_example_dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: OrderedDict([(x, OrderedDict([(start_place, (16, 16)), (start_hour_sin, (16, 16)), (start_hour_cos, (16, 16)), (week_day_sin, (16, 16)), (week_day_cos, (16, 16)), (weekend, (16, 16))])), (y, (16, 16))]), types: OrderedDict([(x, OrderedDict([(start_place, tf.int32), (start_hour_sin, tf.float64), (start_hour_cos, tf.float64), (week_day_sin, tf.float64), (week_day_cos, tf.float64), (weekend, tf.int32)])), (y, tf.int32)])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Dy0rOI5V_F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "8f8ce6d6-ee19-4a70-b499-7458ec00ab5c"
      },
      "source": [
        "preprocessed_example_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('x',\n",
              "              OrderedDict([('start_place',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None)),\n",
              "                           ('start_hour_sin',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n",
              "                           ('start_hour_cos',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n",
              "                           ('week_day_sin',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n",
              "                           ('week_day_cos',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float64, name=None)),\n",
              "                           ('weekend',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])),\n",
              "             ('y', TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-x7fxr63JFr"
      },
      "source": [
        "The ways to feed federated data to TFF in a simulation is simply as a Python list, with each element of the list holding the data of an individual user, whether as a list or as a tf.data.Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOCHe8zE2yNP"
      },
      "source": [
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in tqdm(client_ids)\n",
        "  ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvczxgol3Z1l"
      },
      "source": [
        "Of course, we are in a simulation environment, and all the data is locally available. Typically then, when running simulations, we would simply sample a random subset of the clients to be involved in each round of training, generally different in each round.\n",
        "\n",
        "That said, as you can find out by studying the paper on the Federated Averaging algorithm, achieving convergence in a system with randomly sampled subsets of clients in each round can take a while, and it would be impractical to have to run hundreds of rounds in this interactive tutorial.\n",
        "\n",
        "What we'll do instead is sample the set of clients once, and reuse the same set across rounds to speed up convergence (intentionally over-fitting to these few user's data). We leave it as an exercise for the reader to modify this tutorial to simulate random sampling - it is fairly easy to do (once you do, keep in mind that getting the model to converge may take a while)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM4vvnXK3B4X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "548a4ba2-cc4c-472b-9561-b49e7fe4ec92"
      },
      "source": [
        "# Select the clients\n",
        "sample_clients = client_train_data.client_ids[0:NUM_CLIENTS]\n",
        "\n",
        "# Federate the clients datasets\n",
        "federated_train_data = make_federated_data(client_train_data, sample_clients)\n",
        "federated_val_data = make_federated_data(client_val_data, sample_clients)\n",
        "federated_test_data = make_federated_data(client_test_data, sample_clients)\n",
        "\n",
        "\n",
        "print('\\nNumber of client datasets: {l}'.format(l=len(federated_train_data)))\n",
        "print('First dataset: {d}'.format(d=federated_train_data[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:04<00:00, 21.62it/s]\n",
            "100%|██████████| 100/100 [00:04<00:00, 23.76it/s]\n",
            "100%|██████████| 100/100 [00:03<00:00, 25.75it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of client datasets: 100\n",
            "First dataset: <PrefetchDataset shapes: OrderedDict([(x, OrderedDict([(start_place, (16, 16)), (start_hour_sin, (16, 16)), (start_hour_cos, (16, 16)), (week_day_sin, (16, 16)), (week_day_cos, (16, 16)), (weekend, (16, 16))])), (y, (16, 16))]), types: OrderedDict([(x, OrderedDict([(start_place, tf.int32), (start_hour_sin, tf.float64), (start_hour_cos, tf.float64), (week_day_sin, tf.float64), (week_day_cos, tf.float64), (weekend, tf.int32)])), (y, tf.int32)])>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Oq_He_Osxb"
      },
      "source": [
        "# Federated Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noO2-Ca6_7oj"
      },
      "source": [
        "# All the different places in the dataset\n",
        "indices = np.concatenate((df.pickup_location_id.values, df.dropoff_location_id.values))\n",
        "\n",
        "# Length of the vocabulary of places (e.g. 11)\n",
        "vocab_size = int(np.max(indices) + 1) # + 1 because of 0\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 256\n",
        "\n",
        "# List of numerical column names\n",
        "numerical_column_names = ['start_hour_sin', 'start_hour_cos', 'weekend', 'week_day']\n",
        "\n",
        "# Number of different places\n",
        "number_of_places =  max(locations_sequence.location_id.max(), locations_sequence.location_id.max()) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diSvfc1xMoOH"
      },
      "source": [
        "Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqk9JcOcGpWY"
      },
      "source": [
        "# Create a model\n",
        "def create_keras_model(number_of_places, batch_size):\n",
        "  \n",
        "\t# Shortcut to the layers package\n",
        "  l = tf.keras.layers\n",
        "\t\n",
        "  # List of numeric feature columns to pass to the DenseLayer\n",
        "  numeric_feature_columns = []\n",
        "\n",
        "\n",
        "  # Handling numerical columns \n",
        "  for header in numerical_column_names:\n",
        "\t\t# Append all the numerical columns defined into the list\n",
        "    numeric_feature_columns.append(feature_column.numeric_column(header, shape=N-1))\n",
        "\n",
        "  # Now we need to define an input dictionary.\n",
        "\t# Where the keys are the column names\n",
        "\t# This is a model with multiple inputs, so we need to declare and input layer for each feature\n",
        "  feature_inputs = {\n",
        "    'start_hour_sin': tf.keras.Input((N-1, ), batch_size=batch_size, name='start_hour_sin'),\n",
        "    'start_hour_cos': tf.keras.Input((N-1, ), batch_size=batch_size, name='start_hour_cos'),\n",
        "    'weekend': tf.keras.Input((N-1, ), batch_size=batch_size, name='weekend'),\n",
        "    'week_day_sin': tf.keras.Input((N-1, ), batch_size=batch_size, name='week_day_sin'),\n",
        "    'week_day_cos': tf.keras.Input((N-1, ), batch_size=batch_size, name='week_day_cos'),\n",
        "  }\n",
        "\n",
        "  # We declare two DenseFeature layers, one for the numeric columns which do not require\\ \n",
        "\t# Any training, and one for the categorical. It is easier to do it like this\n",
        "  '''numerical_features = l.DenseFeatures(numeric_feature_columns)(feature_inputs)'''\n",
        "  \n",
        "  # We cannot use anarray of features as always because we have sequences and we cannot match the shape otherwise\n",
        "  # We have to do one by one\n",
        "  start_hour_sin = feature_column.numeric_column(\"start_hour_sin\", shape=(N-1))\n",
        "  hour_sin_feature = l.DenseFeatures(start_hour_sin)(feature_inputs)\n",
        "\n",
        "  start_hour_cos = feature_column.numeric_column(\"start_hour_cos\", shape=(N-1))\n",
        "  hour_cos_feature = l.DenseFeatures(start_hour_cos)(feature_inputs)\n",
        "\n",
        "  weekend = feature_column.numeric_column(\"weekend\", shape=(N-1))\n",
        "  weekend_feature = l.DenseFeatures(weekend)(feature_inputs)\n",
        "\n",
        "  week_day_sin = feature_column.numeric_column(\"week_day_sin\", shape=(N-1))\n",
        "  week_day_sin_feature = l.DenseFeatures(week_day_sin)(feature_inputs)\n",
        "\n",
        "  week_day_cos = feature_column.numeric_column(\"week_day_cos\", shape=(N-1))\n",
        "  week_day_cos_feature = l.DenseFeatures(week_day_cos)(feature_inputs)\n",
        "  \n",
        "\t# We have also to add a dimension to then concatenate\n",
        "  hour_sin_feature = tf.expand_dims(hour_sin_feature, -1)\n",
        "  hour_cos_feature = tf.expand_dims(hour_cos_feature, -1)\n",
        "  weekend_feature = tf.expand_dims(weekend_feature, -1)\n",
        "  week_day_sin_feature = tf.expand_dims(week_day_sin_feature, -1)\n",
        "  week_day_cos_feature = tf.expand_dims(week_day_cos_feature, -1)\n",
        "\n",
        "  # Declare the dictionary for the places sequence as before\n",
        "  sequence_input = {\n",
        "      'start_place': tf.keras.Input((N-1,), batch_size=batch_size, dtype=tf.dtypes.int32, name='start_place') # add batch_size=batch_size in case of stateful GRU\n",
        "  }\n",
        "\n",
        "\n",
        "  # Handling the categorical feature sequence using one-hot\n",
        "  places_one_hot = feature_column.sequence_categorical_column_with_vocabulary_list(\n",
        "      'start_place', [i for i in range(number_of_places)])\n",
        "  \n",
        "  # Embed the one-hot encoding\n",
        "  places_embed = feature_column.embedding_column(places_one_hot, embedding_dim)\n",
        "\n",
        "\n",
        "  # With an input sequence we can't use the DenseFeature layer, we need to use the SequenceFeatures\n",
        "  sequence_features, sequence_length = tf.keras.experimental.SequenceFeatures(places_embed)(sequence_input)\n",
        "\n",
        "  input_sequence = l.Concatenate(axis=2)([ sequence_features, hour_sin_feature, hour_cos_feature, weekend_feature, week_day_sin_feature, week_day_cos_feature])\n",
        "\n",
        "  # Rnn\n",
        "  recurrent = l.GRU(rnn_units,\n",
        "                        batch_size=batch_size, #in case of stateful\n",
        "                        dropout=0.3,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform')(input_sequence)\n",
        "\n",
        "\n",
        "\t# Last layer with an output for each places\n",
        "  dense_1 = layers.Dense(number_of_places)(recurrent)\n",
        "\n",
        "\t# Softmax output layer\n",
        "  output = l.Softmax()(dense_1)\n",
        "\t\n",
        "\t# To return the Model, we need to define it's inputs and outputs\n",
        "\t# In out case, we need to list all the input layers we have defined \n",
        "  inputs = list(feature_inputs.values()) + list(sequence_input.values())\n",
        "\n",
        "\t# Return the Model\n",
        "  return tf.keras.Model(inputs=inputs, outputs=output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3ItQ7X8CYHu"
      },
      "source": [
        "Function to evaluate the federated model on the server.\n",
        "With and without round number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do9fvQMwCT6-"
      },
      "source": [
        "def keras_evaluate(state, round_num, dataset, tb=0):\n",
        "  # Take our global model weights and push them back into a Keras model to\n",
        "  # use its standard `.evaluate()` method.\n",
        "  keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "\t# Load state server parameters into the Keras model\n",
        "  state.model.assign_weights_to(keras_model)\n",
        "  loss, accuracy = keras_model.evaluate(dataset)\n",
        "  if tb == 1:\n",
        "    with eval_summary_writer.as_default():\n",
        "        for name, value in dict(val_metrics).items():\n",
        "          tf.summary.scalar('epoch_loss', loss, step=round_num)\n",
        "          tf.summary.scalar('epoch_sparse_categorical_accuracy', accuracy, step=round_num)\n",
        "  print('\\tEVAL: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_ZDdLJrh864"
      },
      "source": [
        "def keras_evaluate(state, dataset, tb=0):\n",
        "  # Take our global model weights and push them back into a Keras model to\n",
        "  # use its standard `.evaluate()` method.\n",
        "  keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
        "  keras_model.compile(\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "\t# Load state server parameters into the Keras model\n",
        "  state.model.assign_weights_to(keras_model)\n",
        "  loss, accuracy = keras_model.evaluate(dataset)\n",
        "  print('\\tEVAL: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3c6-MzlM0UO"
      },
      "source": [
        "Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShnjsxGb7nXt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7924dde7-2c8e-4aa6-da55-9122c746250b"
      },
      "source": [
        "keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
        "keras_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "keras_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "start_hour_cos (InputLayer)     [(16, 16)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "start_hour_sin (InputLayer)     [(16, 16)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "week_day_cos (InputLayer)       [(16, 16)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "week_day_sin (InputLayer)       [(16, 16)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "weekend (InputLayer)            [(16, 16)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "start_place (InputLayer)        [(16, 16)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_features_5 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
            "                                                                 start_hour_sin[0][0]             \n",
            "                                                                 week_day_cos[0][0]               \n",
            "                                                                 week_day_sin[0][0]               \n",
            "                                                                 weekend[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_features_6 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
            "                                                                 start_hour_sin[0][0]             \n",
            "                                                                 week_day_cos[0][0]               \n",
            "                                                                 week_day_sin[0][0]               \n",
            "                                                                 weekend[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_features_7 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
            "                                                                 start_hour_sin[0][0]             \n",
            "                                                                 week_day_cos[0][0]               \n",
            "                                                                 week_day_sin[0][0]               \n",
            "                                                                 weekend[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_features_8 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
            "                                                                 start_hour_sin[0][0]             \n",
            "                                                                 week_day_cos[0][0]               \n",
            "                                                                 week_day_sin[0][0]               \n",
            "                                                                 weekend[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_features_9 (DenseFeatures (16, 16)             0           start_hour_cos[0][0]             \n",
            "                                                                 start_hour_sin[0][0]             \n",
            "                                                                 week_day_cos[0][0]               \n",
            "                                                                 week_day_sin[0][0]               \n",
            "                                                                 weekend[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequence_features_1 (SequenceFe ((None, None, 256),  67584       start_place[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_5 (TFOpLambda)   (16, 16, 1)          0           dense_features_5[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_6 (TFOpLambda)   (16, 16, 1)          0           dense_features_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_7 (TFOpLambda)   (16, 16, 1)          0           dense_features_7[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_8 (TFOpLambda)   (16, 16, 1)          0           dense_features_8[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_9 (TFOpLambda)   (16, 16, 1)          0           dense_features_9[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (16, 16, 261)        0           sequence_features_1[0][0]        \n",
            "                                                                 tf.expand_dims_5[0][0]           \n",
            "                                                                 tf.expand_dims_6[0][0]           \n",
            "                                                                 tf.expand_dims_7[0][0]           \n",
            "                                                                 tf.expand_dims_8[0][0]           \n",
            "                                                                 tf.expand_dims_9[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (16, 16, 256)        398592      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (16, 16, 264)        67848       gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "softmax_1 (Softmax)             (16, 16, 264)        0           dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 534,024\n",
            "Trainable params: 534,024\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK5r_WJra672"
      },
      "source": [
        "TFF serializes all TensorFlow computations so they can potentially be run in a non-Python environment (even though at the moment, only a simulation runtime implemented in Python is available). Even though we are running in eager mode, (TF 2.0), currently TFF serializes TensorFlow computations by constructing the necessary ops inside the context of a \"with tf.Graph.as_default()\" statement. Thus, we need to provide a function that TFF can use to introduce our model into a graph it controls. We do this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgfd00ifa7ry"
      },
      "source": [
        "# Clone the keras_model inside `create_tff_model()`, which TFF will\n",
        "# call to produce a new copy of the model inside the graph that it will \n",
        "# serialize. Note: we want to construct all the necessary objects we'll need \n",
        "# _inside_ this method.\n",
        "def create_tff_model():\n",
        "  # TFF uses an `input_spec` so it knows the types and shapes\n",
        "  # that your model expects.\n",
        "  input_spec = preprocessed_example_dataset.element_spec\n",
        "  keras_model_clone = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model_clone,\n",
        "      input_spec=input_spec,\n",
        "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs7Dj5tOcEOS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f0de70b2-c742-4025-ef96-a07c1ea2ff8e"
      },
      "source": [
        "preprocessed_example_dataset.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('x',\n",
              "              OrderedDict([('start_place',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None)),\n",
              "                           ('start_hour_sin',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float32, name=None)),\n",
              "                           ('start_hour_cos',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float32, name=None)),\n",
              "                           ('week_day_sin',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float32, name=None)),\n",
              "                           ('week_day_cos',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.float32, name=None)),\n",
              "                           ('weekend',\n",
              "                            TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])),\n",
              "             ('y', TensorSpec(shape=(16, 16), dtype=tf.int32, name=None))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ywd5cFRbYG-"
      },
      "source": [
        "We use a compiled Keras model to perform standard (non-federated) evaluation after each round of federated training. This is useful for research purposes when doing simulated federated learning and there is a standard test dataset.\n",
        "\n",
        "In a realistic production setting this same technique might be used to take models trained with federated learning and evaluate them on a centralized benchmark dataset for testing or quality assurance purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9swVmhFTbYte"
      },
      "source": [
        "# This command builds all the TensorFlow graphs and serializes them: \n",
        "fed_avg = tff.learning.build_federated_averaging_process(\n",
        "    model_fn=create_tff_model,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.002),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Adam(learning_rate=0.06))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_gUHP6WbkX_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cc691ef6-fabe-4b48-9206-47a1b8d83ec2"
      },
      "source": [
        "# EMBEDDED\n",
        "str(fed_avg.initialize.type_signature)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'( -> <model=<trainable=<float32[264,256],float32[261,768],float32[256,768],float32[2,768],float32[256,264],float32[264]>,non_trainable=<>>,optimizer_state=<int64,float32[264,256],float32[261,768],float32[256,768],float32[2,768],float32[256,264],float32[264],float32[264,256],float32[261,768],float32[256,768],float32[2,768],float32[256,264],float32[264]>,delta_aggregate_state=<>,model_broadcast_state=<>>@SERVER)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHZHKAs7qHBD"
      },
      "source": [
        "state = fed_avg.initialize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzyL7EwcqJVS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "2b813fd5-844c-4421-da96-a8718c89336a"
      },
      "source": [
        "# State Embedded\n",
        "str(state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ServerState(model=ModelWeights(trainable=[array([[-0.02949737,  0.08731643, -0.03885576, ..., -0.00102725,\\n         0.05004363, -0.0227446 ],\\n       [ 0.03515191,  0.01044064, -0.02936067, ...,  0.04273547,\\n         0.00609752, -0.0648926 ],\\n       [-0.08920676, -0.00609571, -0.0046847 , ...,  0.11479594,\\n        -0.04155695,  0.11941169],\\n       ...,\\n       [ 0.02493653, -0.08216283, -0.0200722 , ..., -0.00485787,\\n        -0.07801205, -0.02115065],\\n       [-0.11104205, -0.11122086, -0.00554694, ...,  0.04946372,\\n        -0.0665529 ,  0.03813975],\\n       [ 0.00725875,  0.01706488, -0.11773968, ..., -0.06014968,\\n         0.00707063,  0.03465395]], dtype=float32), array([[ 0.06532948, -0.04734532,  0.06585801, ...,  0.02837193,\\n        -0.05359245,  0.06506432],\\n       [ 0.0697296 ,  0.03222064,  0.07541501, ..., -0.07411402,\\n         0.02461872,  0.01021443],\\n       [-0.03110623,  0.0168295 , -0.01105376, ...,  0.04667696,\\n        -0.04568096,  0.03146423],\\n       ...,\\n       [ 0.00289839,  0.06980544,  0.04867569, ..., -0.05826876,\\n         0.03178434,  0.05979285],\\n       [ 0.04019398, -0.0705713 ,  0.04435576, ..., -0.01149563,\\n         0.01087539, -0.00260193],\\n       [-0.04764293, -0.00777876, -0.03960816, ..., -0.01220173,\\n        -0.01834633,  0.03681693]], dtype=float32), array([[ 0.03104287, -0.0333206 ,  0.04445447, ..., -0.05250791,\\n        -0.06156345,  0.03603788],\\n       [-0.06150913, -0.06172819, -0.03760547, ...,  0.07314775,\\n        -0.0574334 , -0.06270121],\\n       [-0.02341367, -0.07128881,  0.01904893, ...,  0.03921974,\\n        -0.02518719,  0.05704201],\\n       ...,\\n       [ 0.03085856, -0.05146817,  0.02953935, ...,  0.07527842,\\n        -0.01494225,  0.07079262],\\n       [-0.04634549,  0.07174595,  0.00758125, ...,  0.02225892,\\n        -0.04417451, -0.02504298],\\n       [ 0.03303659,  0.01641467, -0.05784808, ..., -0.01249398,\\n        -0.00412937, -0.0718685 ]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[-0.0564671 ,  0.07228284, -0.05548103, ...,  0.04885424,\\n        -0.01548593,  0.06899691],\\n       [ 0.04898926,  0.03090581, -0.08952145, ...,  0.06616069,\\n         0.07477198,  0.04133894],\\n       [ 0.01363124, -0.09046936,  0.08965486, ..., -0.07984422,\\n         0.06612227, -0.02106468],\\n       ...,\\n       [-0.0979211 ,  0.08315098,  0.03283013, ..., -0.08118735,\\n        -0.01375384,  0.01343906],\\n       [-0.00828467,  0.05078178, -0.06728951, ...,  0.05491403,\\n        -0.09086411,  0.08601303],\\n       [ 0.01754989,  0.09830805,  0.05538452, ...,  0.00314973,\\n        -0.01104097, -0.0933435 ]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)], non_trainable=[]), optimizer_state=[0, array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([[0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       ...,\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.],\\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\\n       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)], delta_aggregate_state=(), model_broadcast_state=())'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mblk51KhRzeh"
      },
      "source": [
        "evaluation = tff.learning.build_federated_evaluation(model_fn=create_tff_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBLe93Dfm4sA"
      },
      "source": [
        "Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jaGN6Tfm4R5"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "# Log directory where we want to save the logs\n",
        "train_logdir = baseURL + 'tb_final/fl_rnn/train'\n",
        "val_logdir = baseURL + 'tb_final/fl_rnn/val'\n",
        "#eval_logdir = baseURL + 'tb_final/fl_rnn/eval'\n",
        "\n",
        "# Summary writer to save the logs\n",
        "train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
        "val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
        "#eval_summary_writer = tf.summary.create_file_writer(eval_logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDVjgsQIvMkP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e577da9-9f39-40ce-f10d-6d22beb804aa"
      },
      "source": [
        "# Run this cell to clean your directory of old output for future graphs from this directory.\n",
        "!rm -R '/content/gdrive/My Drive/tb/fl_rnn/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove '/content/gdrive/My Drive/tb/fl_rnn/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLTlfunoO0Yd"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drbv1ZxLnngJ"
      },
      "source": [
        "Then we just need to wrap our training loop with the summary_writer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMF6528LqMmg"
      },
      "source": [
        "NUM_ROUNDS = 15\n",
        "#Plot the relevant scalar metrics with the same summary writer.\n",
        "#@test {\"skip\": true}\n",
        "with train_summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS + 1):\n",
        "    print('Round {r}'.format(r=round_num))\n",
        "\n",
        "    # Uncomment to simulate sparse availabily of clients\n",
        "    # data_for_this_round = sample(federated_train_data)\n",
        "\n",
        "    state, metrics = fed_avg.next(state, federated_train_data)\n",
        "\n",
        "    # Federated train\n",
        "    train_metrics = metrics['train']\n",
        "    print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
        "\n",
        "    # Federated evaluation\n",
        "    val_metrics = evaluation(state.model, federated_val_data)\n",
        "    print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
        "    \n",
        "    # Centralized Evaluation\n",
        "    keras_evaluate(state, round_num, val_dataset, tb=1)\n",
        "    print(' ')\n",
        "\n",
        "    print('\\twriting..')\n",
        "    # Iterate across the train metrics and write their data\n",
        "    for name, value in dict(train_metrics).items():\n",
        "      # print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
        "      tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
        "      \n",
        "    # Validation metrics\n",
        "    with val_summary_writer.as_default():\n",
        "      for name, value in dict(val_metrics).items():\n",
        "        # print('\\twriting..')\n",
        "         #print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
        "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
        "\n",
        "train_summary_writer.close()\n",
        "val_summary_writer.close()\n",
        "eval_summary_writer.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsAUXDwrNdHY"
      },
      "source": [
        "Training with early-stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTci_WPx8qVb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36dc7051-b360-4b94-eb5d-613deda81903"
      },
      "source": [
        "# Local model for evaluation\n",
        "'''keras_model = create_keras_model(number_of_places, batch_size=BATCH_SIZE)\n",
        "keras_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])'''\n",
        "\n",
        "tolerance = 7\n",
        "best_state = 0\n",
        "lowest_loss = 100.00\n",
        "stop = tolerance\n",
        "\n",
        "NUM_ROUNDS = 40\n",
        "with train_summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS + 1):\n",
        "    print('Round {r}'.format(r=round_num))\n",
        "\n",
        "    # Uncomment to simulate sparse availabily of clients\n",
        "    # train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 20, NUM_CLIENTS)\n",
        "\n",
        "    state, metrics = fed_avg.next(state, federated_train_data)\n",
        "\n",
        "    train_metrics = metrics['train']\n",
        "    print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
        "\n",
        "    val_metrics = evaluation(state.model, federated_val_data)\n",
        "    print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
        "    \n",
        "    # Check for decreasing validation loss\n",
        "    if lowest_loss > val_metrics['loss']:\n",
        "      print('\\tSaving best model..')\n",
        "      lowest_loss = val_metrics['loss']\n",
        "      best_state = state\n",
        "      stop = tolerance - 1 \n",
        "    else:\n",
        "      stop = stop - 1\n",
        "      if stop <= 0:\n",
        "        print('\\tEarly stopping...')\n",
        "        break;\n",
        "    \n",
        "    # keras_evaluate(state, round_num, val_dataset)\n",
        "    # Evaluation\n",
        "    '''state.model.assign_weights_to(keras_model)\n",
        "    loss, accuracy = keras_model.evaluate(val_dataset)\n",
        "    print('\\tEVAL: loss={l:.3f}, accuracy={a:.3f}'.format(l=loss, a=accuracy))'''\n",
        "    print(' ')\n",
        "    print('\\twriting..')\n",
        "\n",
        "    # Iterate across the metrics and write their data\n",
        "    for name, value in dict(train_metrics).items():\n",
        "      # print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
        "      tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
        "\n",
        "    with val_summary_writer.as_default():\n",
        "      for name, value in dict(val_metrics).items():\n",
        "        # print('\\twriting..')\n",
        "        # print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
        "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
        "\n",
        "train_summary_writer.close()\n",
        "val_summary_writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round 1\n",
            "\tTrain: loss=4.366, accuracy=0.149\n",
            "\tValidation: loss=4.658, accuracy=0.193\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 2\n",
            "\tTrain: loss=4.252, accuracy=0.200\n",
            "\tValidation: loss=4.599, accuracy=0.090\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 3\n",
            "\tTrain: loss=3.761, accuracy=0.205\n",
            "\tValidation: loss=4.906, accuracy=0.133\n",
            " \n",
            "\twriting..\n",
            "Round 4\n",
            "\tTrain: loss=3.646, accuracy=0.229\n",
            "\tValidation: loss=3.795, accuracy=0.238\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 5\n",
            "\tTrain: loss=3.469, accuracy=0.223\n",
            "\tValidation: loss=3.753, accuracy=0.228\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 6\n",
            "\tTrain: loss=3.456, accuracy=0.230\n",
            "\tValidation: loss=3.658, accuracy=0.249\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 7\n",
            "\tTrain: loss=3.431, accuracy=0.236\n",
            "\tValidation: loss=3.842, accuracy=0.242\n",
            " \n",
            "\twriting..\n",
            "Round 8\n",
            "\tTrain: loss=3.477, accuracy=0.232\n",
            "\tValidation: loss=3.701, accuracy=0.246\n",
            " \n",
            "\twriting..\n",
            "Round 9\n",
            "\tTrain: loss=3.444, accuracy=0.234\n",
            "\tValidation: loss=3.591, accuracy=0.242\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 10\n",
            "\tTrain: loss=3.403, accuracy=0.238\n",
            "\tValidation: loss=3.449, accuracy=0.243\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 11\n",
            "\tTrain: loss=3.362, accuracy=0.241\n",
            "\tValidation: loss=3.503, accuracy=0.269\n",
            " \n",
            "\twriting..\n",
            "Round 12\n",
            "\tTrain: loss=3.366, accuracy=0.241\n",
            "\tValidation: loss=3.455, accuracy=0.283\n",
            " \n",
            "\twriting..\n",
            "Round 13\n",
            "\tTrain: loss=3.361, accuracy=0.241\n",
            "\tValidation: loss=3.461, accuracy=0.273\n",
            " \n",
            "\twriting..\n",
            "Round 14\n",
            "\tTrain: loss=3.365, accuracy=0.245\n",
            "\tValidation: loss=3.449, accuracy=0.263\n",
            " \n",
            "\twriting..\n",
            "Round 15\n",
            "\tTrain: loss=3.362, accuracy=0.246\n",
            "\tValidation: loss=3.425, accuracy=0.265\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 16\n",
            "\tTrain: loss=3.352, accuracy=0.247\n",
            "\tValidation: loss=3.422, accuracy=0.272\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 17\n",
            "\tTrain: loss=3.352, accuracy=0.247\n",
            "\tValidation: loss=3.420, accuracy=0.287\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 18\n",
            "\tTrain: loss=3.357, accuracy=0.246\n",
            "\tValidation: loss=3.427, accuracy=0.289\n",
            " \n",
            "\twriting..\n",
            "Round 19\n",
            "\tTrain: loss=3.365, accuracy=0.247\n",
            "\tValidation: loss=3.428, accuracy=0.289\n",
            " \n",
            "\twriting..\n",
            "Round 20\n",
            "\tTrain: loss=3.362, accuracy=0.249\n",
            "\tValidation: loss=3.407, accuracy=0.285\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 21\n",
            "\tTrain: loss=3.350, accuracy=0.251\n",
            "\tValidation: loss=3.409, accuracy=0.279\n",
            " \n",
            "\twriting..\n",
            "Round 22\n",
            "\tTrain: loss=3.351, accuracy=0.251\n",
            "\tValidation: loss=3.424, accuracy=0.286\n",
            " \n",
            "\twriting..\n",
            "Round 23\n",
            "\tTrain: loss=3.349, accuracy=0.251\n",
            "\tValidation: loss=3.410, accuracy=0.284\n",
            " \n",
            "\twriting..\n",
            "Round 24\n",
            "\tTrain: loss=3.344, accuracy=0.252\n",
            "\tValidation: loss=3.411, accuracy=0.267\n",
            " \n",
            "\twriting..\n",
            "Round 25\n",
            "\tTrain: loss=3.343, accuracy=0.253\n",
            "\tValidation: loss=3.411, accuracy=0.267\n",
            " \n",
            "\twriting..\n",
            "Round 26\n",
            "\tTrain: loss=3.341, accuracy=0.254\n",
            "\tValidation: loss=3.386, accuracy=0.292\n",
            "\tSaving best model..\n",
            " \n",
            "\twriting..\n",
            "Round 27\n",
            "\tTrain: loss=3.342, accuracy=0.253\n",
            "\tValidation: loss=3.403, accuracy=0.301\n",
            " \n",
            "\twriting..\n",
            "Round 28\n",
            "\tTrain: loss=3.351, accuracy=0.251\n",
            "\tValidation: loss=3.422, accuracy=0.301\n",
            " \n",
            "\twriting..\n",
            "Round 29\n",
            "\tTrain: loss=3.347, accuracy=0.252\n",
            "\tValidation: loss=3.396, accuracy=0.284\n",
            " \n",
            "\twriting..\n",
            "Round 30\n",
            "\tTrain: loss=3.338, accuracy=0.254\n",
            "\tValidation: loss=3.423, accuracy=0.256\n",
            " \n",
            "\twriting..\n",
            "Round 31\n",
            "\tTrain: loss=3.340, accuracy=0.256\n",
            "\tValidation: loss=3.413, accuracy=0.284\n",
            " \n",
            "\twriting..\n",
            "Round 32\n",
            "\tTrain: loss=3.340, accuracy=0.255\n",
            "\tValidation: loss=3.410, accuracy=0.303\n",
            "\tEarly stopping...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bka_CJCN8Ce"
      },
      "source": [
        "Test best saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V-qfu-LZlQX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e810b5f-90fc-408a-f999-3170dd7096c1"
      },
      "source": [
        "# Test the model\n",
        "test_metrics = evaluation(best_state.model, federated_test_data)\n",
        "print('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEvaluation: loss=3.414, accuracy=0.290\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3goiwxoC0GN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "22b17eef-8498-4215-d1d3-079fee0b3d94"
      },
      "source": [
        "# Centralized test\n",
        "keras_evaluate(best_state, dataset=test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43/43 [==============================] - 1s 15ms/step - loss: 3.5584 - sparse_categorical_accuracy: 0.2327\n",
            "\tEVAL: loss=3.446, accuracy=0.266\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkYXS--qN_le"
      },
      "source": [
        "Test last model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLrl_0QwRAO7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1311cba-bad0-4bfa-f1b6-fc5ec3cda060"
      },
      "source": [
        "# Test the model\n",
        "test_metrics = evaluation(state.model, federated_test_data)\n",
        "print('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEvaluation: loss=3.431, accuracy=0.300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM2UUiSLRAO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dfbfc6a1-2f2a-4bb4-93cc-baec79870f88"
      },
      "source": [
        "# Centralized test\n",
        "keras_evaluate(state, dataset=test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "43/43 [==============================] - 1s 14ms/step - loss: 3.5467 - sparse_categorical_accuracy: 0.2343\n",
            "\tEVAL: loss=3.411, accuracy=0.277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAO-inmtOCYK"
      },
      "source": [
        "Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chncva45qDiP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "6ecb39d6-b47e-4fcc-8e16-8e654748a1ff"
      },
      "source": [
        "import datetime, os\n",
        "!kill 719 # If you want to kill the process with the PID\n",
        "%reload_ext tensorboard\n",
        "\n",
        "# We cannot use the logdir variable, we need to define it manually\n",
        "%tensorboard --logdir '/content/gdrive/My Drive/NYC Dataset/tb/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: kill: (719) - No such process\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with -11).\n",
              "Contents of stderr:\n",
              "2020-09-13 18:14:28.443116: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
              "2020-09-13 18:14:28.443190: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
              "Traceback (most recent call last):\n",
              "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
              "    sys.exit(run_main())\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 75, in run_main\n",
              "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
              "    _run_main(main, args)\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
              "    sys.exit(main(argv))\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 289, in main\n",
              "    return runner(self.flags) or 0\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 305, in _run_serve_subcommand\n",
              "    server = self._make_server()\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 415, in _make_server\n",
              "    ingester.deprecated_multiplexer,\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 149, in TensorBoardWSGIApp\n",
              "    experimental_middlewares,\n",
              "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 257, in __init__\n",
              "    \"Duplicate plugins for name %s\" % plugin.plugin_name\n",
              "ValueError: Duplicate plugins for name projector"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58sCHv3kkpBV"
      },
      "source": [
        "### Training by sampling clients at each round"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7fXx_3mypnK"
      },
      "source": [
        "# sample a subset of clients\n",
        "# federated_data is a tuple with the train and the validation data\n",
        "def sample(federate_data, n, n_clients):\n",
        "  client_ids = np.random.choice(n_clients, n, replace=False).astype(int)\n",
        "  return [federate_data[0][i] for i in client_ids], [federate_data[1][i] for i in client_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNLo4IS7kRb"
      },
      "source": [
        "Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSZ10fVz7kRc"
      },
      "source": [
        "#@test {\"skip\": true}\n",
        "# Log directory where we want to save the logs\n",
        "train_logdir = baseURL + 'tb/fl_rnn_sampling_new/train'\n",
        "val_logdir = baseURL + 'tb/fl_rnn_sampling_new/val'\n",
        "eval_logdir = baseURL + 'tb/fl_rnn_sampling_new/eval'\n",
        "\n",
        "# Summary writer to save the logs\n",
        "train_summary_writer = tf.summary.create_file_writer(train_logdir)\n",
        "val_summary_writer = tf.summary.create_file_writer(val_logdir)\n",
        "eval_summary_writer = tf.summary.create_file_writer(eval_logdir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuuEVKxJhK3l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "670ad6c6-e01f-449a-ff2b-684aca2076a8"
      },
      "source": [
        "NUM_ROUNDS = 10\n",
        "with train_summary_writer.as_default():\n",
        "  for round_num in range(1, NUM_ROUNDS + 1):\n",
        "    print('Round {r}'.format(r=round_num))\n",
        "    train_data_for_this_round, val_data_for_this_round = sample((federated_train_data, federated_val_data), 50, NUM_CLIENTS)\n",
        "    state, metrics = fed_avg.next(state, train_data_for_this_round)\n",
        "    train_metrics = metrics['train']\n",
        "    print('\\tTrain: loss={l:.3f}, accuracy={a:.3f}'.format(l=train_metrics['loss'], a=train_metrics['sparse_categorical_accuracy']))\n",
        "    val_metrics = evaluation(state.model, federated_val_data)\n",
        "    print('\\tValidation: loss={l:.3f}, accuracy={a:.3f}'.format( l=val_metrics['loss'], a=val_metrics['sparse_categorical_accuracy']))\n",
        "    #keras_evaluate(state, round_num, val_dataset, tb=1)\n",
        "    print(' ')\n",
        "\n",
        "    print('\\twriting..')  \n",
        "    # Iterate across the metrics and write their data\n",
        "    for name, value in dict(train_metrics).items():\n",
        "      \n",
        "       #print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
        "      tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
        "\n",
        "    with val_summary_writer.as_default():\n",
        "      for name, value in dict(val_metrics).items():\n",
        "        #print('\\twriting..')\n",
        "        #print('\\tname: {}, value:{}, step={}'.format(name,value,round_num))\n",
        "        tf.summary.scalar('epoch_'+name, value, step=round_num)\n",
        "        \n",
        "train_summary_writer.close()\n",
        "val_summary_writer.close()\n",
        "eval_summary_writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round 31\n",
            "\tTrain: loss=3.407, accuracy=0.244\n",
            "\tValidation: loss=3.416, accuracy=0.281\n",
            " \n",
            "\twriting..\n",
            "Round 32\n",
            "\tTrain: loss=3.378, accuracy=0.250\n",
            "\tValidation: loss=3.430, accuracy=0.264\n",
            " \n",
            "\twriting..\n",
            "Round 33\n",
            "\tTrain: loss=3.406, accuracy=0.246\n",
            "\tValidation: loss=3.418, accuracy=0.276\n",
            " \n",
            "\twriting..\n",
            "Round 34\n",
            "\tTrain: loss=3.395, accuracy=0.245\n",
            "\tValidation: loss=3.413, accuracy=0.294\n",
            " \n",
            "\twriting..\n",
            "Round 35\n",
            "\tTrain: loss=3.404, accuracy=0.245\n",
            "\tValidation: loss=3.436, accuracy=0.296\n",
            " \n",
            "\twriting..\n",
            "Round 36\n",
            "\tTrain: loss=3.425, accuracy=0.245\n",
            "\tValidation: loss=3.422, accuracy=0.294\n",
            " \n",
            "\twriting..\n",
            "Round 37\n",
            "\tTrain: loss=3.413, accuracy=0.244\n",
            "\tValidation: loss=3.438, accuracy=0.264\n",
            " \n",
            "\twriting..\n",
            "Round 38\n",
            "\tTrain: loss=3.412, accuracy=0.247\n",
            "\tValidation: loss=3.457, accuracy=0.250\n",
            " \n",
            "\twriting..\n",
            "Round 39\n",
            "\tTrain: loss=3.426, accuracy=0.244\n",
            "\tValidation: loss=3.436, accuracy=0.288\n",
            " \n",
            "\twriting..\n",
            "Round 40\n",
            "\tTrain: loss=3.419, accuracy=0.242\n",
            "\tValidation: loss=3.461, accuracy=0.299\n",
            " \n",
            "\twriting..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbgNpWlbhBuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3310b56-074f-4493-ed7d-c743c002c00d"
      },
      "source": [
        "# test the model\n",
        "test_metrics = evaluation(state.model, federated_test_data)\n",
        "print('\\tEvaluation: loss={l:.3f}, accuracy={a:.3f}'.format( l=test_metrics['loss'], a=test_metrics['sparse_categorical_accuracy']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tEvaluation: loss=3.489, accuracy=0.298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWxZwBt6ovBF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3c78eb7f-ef2a-4f6a-98fa-58b760fd2fda"
      },
      "source": [
        "# Centralized test\n",
        "keras_evaluate(state, round_num=round_num, dataset=test_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42/42 [==============================] - 1s 16ms/step - loss: 3.5350 - sparse_categorical_accuracy: 0.2337\n",
            "\tEVAL: loss=3.463, accuracy=0.272\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}